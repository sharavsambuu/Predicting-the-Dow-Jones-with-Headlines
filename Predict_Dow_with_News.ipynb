{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Dow Jones with News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to use the top, daily news headlines from Reddit to predict the movement of the Dow Jones Industrial Average. The data for this project spans from 2008-08-08 to 2016-07-01, and is from this [Kaggle dataset](https://www.kaggle.com/aaron7sun/stocknews). \n",
    "\n",
    "The news from a given day will be used to predict the difference in opening price between that day, and the following day. This method is chosen because it should allow all of the day's news to be incorporated into the price compared to closing price, which could only incorporate the day's morning and afternoon news.\n",
    "\n",
    "For this project, we will use GloVe to create our word embeddings and CNNs followed by LSTMs to build our model. This model is based off the work done in this paper https://www.aclweb.org/anthology/C/C16/C16-1229.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import median_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras.layers import Dropout, Activation, Embedding, Convolution1D, MaxPooling1D, Input, Dense, merge, \\\n",
    "                         BatchNormalization, Flatten, Reshape, Concatenate\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = pd.read_csv(\"DowJones.csv\")\n",
    "news = pd.read_csv(\"News.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>17924.240234</td>\n",
       "      <td>18002.380859</td>\n",
       "      <td>17916.910156</td>\n",
       "      <td>17949.369141</td>\n",
       "      <td>82160000</td>\n",
       "      <td>17949.369141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>17712.759766</td>\n",
       "      <td>17930.609375</td>\n",
       "      <td>17711.800781</td>\n",
       "      <td>17929.990234</td>\n",
       "      <td>133030000</td>\n",
       "      <td>17929.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17704.509766</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17694.679688</td>\n",
       "      <td>106380000</td>\n",
       "      <td>17694.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>112190000</td>\n",
       "      <td>17409.720703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17063.080078</td>\n",
       "      <td>17140.240234</td>\n",
       "      <td>138740000</td>\n",
       "      <td>17140.240234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Open          High           Low         Close  \\\n",
       "0  2016-07-01  17924.240234  18002.380859  17916.910156  17949.369141   \n",
       "1  2016-06-30  17712.759766  17930.609375  17711.800781  17929.990234   \n",
       "2  2016-06-29  17456.019531  17704.509766  17456.019531  17694.679688   \n",
       "3  2016-06-28  17190.509766  17409.720703  17190.509766  17409.720703   \n",
       "4  2016-06-27  17355.210938  17355.210938  17063.080078  17140.240234   \n",
       "\n",
       "      Volume     Adj Close  \n",
       "0   82160000  17949.369141  \n",
       "1  133030000  17929.990234  \n",
       "2  106380000  17694.679688  \n",
       "3  112190000  17409.720703  \n",
       "4  138740000  17140.240234  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Volume       0\n",
       "Adj Close    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date    0\n",
       "News    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               News\n",
       "0  2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
       "1  2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
       "2  2016-07-01  The president of France says if Brexit won, so...\n",
       "3  2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
       "4  2016-07-01  100+ Nobel laureates urge Greenpeace to stop o..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1989, 7)\n",
      "(73608, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dj.shape)\n",
    "print(news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989\n",
      "2943\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of unique dates. We want matching values.\n",
    "print(len(set(dj.Date)))\n",
    "print(len(set(news.Date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the extra dates that are in news\n",
    "news = news[news.Date.isin(dj.Date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989\n",
      "1989\n"
     ]
    }
   ],
   "source": [
    "print(len(set(dj.Date)))\n",
    "print(len(set(news.Date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference in opening prices between the following and current day.\n",
    "# The model will try to predict how much the Open value will change beased on the news.\n",
    "dj = dj.set_index('Date').diff(periods=1)\n",
    "dj['Date'] = dj.index\n",
    "dj = dj.reset_index(drop=True)\n",
    "# Remove unneeded features\n",
    "dj = dj.drop(['High','Low','Close','Volume','Adj Close'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-211.480468</td>\n",
       "      <td>2016-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-256.740235</td>\n",
       "      <td>2016-06-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-265.509765</td>\n",
       "      <td>2016-06-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164.701172</td>\n",
       "      <td>2016-06-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open        Date\n",
       "0         NaN  2016-07-01\n",
       "1 -211.480468  2016-06-30\n",
       "2 -256.740235  2016-06-29\n",
       "3 -265.509765  2016-06-28\n",
       "4  164.701172  2016-06-27"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove top row since it has a null value.\n",
    "dj = dj[dj.Open.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open    0\n",
       "Date    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are any more null values.\n",
    "dj.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the opening prices and their corresponding daily headlines from the news\n",
    "price = []\n",
    "headlines = []\n",
    "\n",
    "for row in dj.iterrows():\n",
    "    daily_headlines = []\n",
    "    date = row[1]['Date']\n",
    "    price.append(row[1]['Open'])\n",
    "    for row_ in news[news.Date==date].iterrows():\n",
    "        daily_headlines.append(row_[1]['News'])\n",
    "    \n",
    "    # Track progress\n",
    "    headlines.append(daily_headlines)\n",
    "    if len(price) % 500 == 0:\n",
    "        print(len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988\n",
      "1988\n"
     ]
    }
   ],
   "source": [
    "# Compare lengths to ensure they are the same\n",
    "print(len(price))\n",
    "print(len(headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "22\n",
      "24.996478873239436\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of headlines for each day\n",
    "print(max(len(i) for i in headlines))\n",
    "print(min(len(i) for i in headlines))\n",
    "print(np.mean([len(i) for i in headlines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'0,0', '00', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'\\$', ' $ ', text)\n",
    "    text = re.sub(r'u s ', ' united states ', text)\n",
    "    text = re.sub(r'u n ', ' united nations ', text)\n",
    "    text = re.sub(r'u k ', ' united kingdom ', text)\n",
    "    text = re.sub(r'j k ', ' jk ', text)\n",
    "    text = re.sub(r' s ', ' ', text)\n",
    "    text = re.sub(r' yr ', ' year ', text)\n",
    "    text = re.sub(r' l g b t ', ' lgbt ', text)\n",
    "    text = re.sub(r'0km ', '0 km ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the headlines\n",
    "clean_headlines = []\n",
    "\n",
    "for daily_headlines in headlines:\n",
    "    clean_daily_headlines = []\n",
    "    for headline in daily_headlines:\n",
    "        clean_daily_headlines.append(clean_text(headline))\n",
    "    clean_headlines.append(clean_daily_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jamaica proposes marijuana dispensers tourists airports following legalisation kiosks desks would give people license purchase 2 ounces drug use stay',\n",
       " 'stephen hawking says pollution stupidity still biggest threats mankind certainly become less greedy less stupid treatment environment past decade',\n",
       " 'boris johnson says run tory party leadership',\n",
       " 'six gay men ivory coast abused forced flee homes pictured signing condolence book victims recent attack gay nightclub florida',\n",
       " 'switzerland denies citizenship muslim immigrant girls refused swim boys report',\n",
       " 'palestinian terrorist stabs israeli teen girl death bedroom',\n",
       " 'puerto rico default $ 1 billion debt friday',\n",
       " 'republic ireland fans awarded medal sportsmanship paris mayor',\n",
       " 'afghan suicide bomber kills 40 bbc news',\n",
       " 'us airstrikes kill least 250 isis fighters convoy outside fallujah official says',\n",
       " 'turkish cop took istanbul gunman hailed hero',\n",
       " 'cannabis compounds could treat alzheimer removing plaque forming proteins brain cells research suggests',\n",
       " 'japan top court approved blanket surveillance country muslims made us terrorist suspects never anything wrong says japanese muslim mohammed fujita',\n",
       " 'cia gave romania millions host secret prisons',\n",
       " 'groups urge united nations suspend saudi arabia rights council',\n",
       " 'googles free wifi indian railway stations better countrys paid services',\n",
       " 'mounting evidence suggests hobbits wiped modern humans ancestors 50000 years ago',\n",
       " 'men carried tuesday terror attack istanbul ataturk airport russia uzbekistan kyrgyzstan turkish offical said',\n",
       " 'calls suspend saudi arabia un human rights council military aggresion yemen',\n",
       " '100 nobel laureates call greenpeace anti gmo obstruction developing world',\n",
       " 'british pedophile sentenced 85 years us trafficking child abuse images domminich shaw kingpin sexual violence children sent dozens images online discussed plans assault kill child probation',\n",
       " 'us permitted 1 200 offshore fracks gulf mexico 2010 2014 allowed 72 billion gallons chemical discharge 2014',\n",
       " 'swimming ridicule french beach police carry guns swimming trunks police lifeguards frances busiest beaches carry guns bullet proof vests first time summer amid fears terrorists could target holidaymakers',\n",
       " 'uefa says minutes silence istanbul victims euro 2016 turkey already eliminated',\n",
       " 'law enforcement sources gun used paris terrorist attacks came phoenix']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at some headlines to ensure everything was cleaned well\n",
    "clean_headlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 35190\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "for date in clean_headlines:\n",
    "    for headline in date:\n",
    "        for word in headline.split():\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 1\n",
    "            else:\n",
    "                word_counts[word] += 1\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 2196016\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe's embeddings\n",
    "embeddings_index = {}\n",
    "with open('./glove.840B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from GloVe: 47\n",
      "Percent of words that are missing from vocabulary: 0.13%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from GloVe, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 10\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from GloVe:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique Words: 35190\n",
      "Number of Words we will use: 31265\n",
      "Percent of Words we will use: 88.85%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear â‰¥ threshold or are in GloVe\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total Number of Unique Words:\", len(word_counts))\n",
    "print(\"Number of Words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of Words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31265\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match GloVe's vectors.\n",
    "embedding_dim = 300\n",
    "\n",
    "nb_words = len(vocab_to_int)\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in GloVe, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The embeddings will be updated as the model trains, so our new 'random' embeddings will be more accurate by the end of training. This is also why we want to only use words that appear at least 10 times. By having the model see the word numerous timesm it will be better able to understand what it means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 615989\n",
      "Total number of UNKs in headlines: 5262\n",
      "Percent of words that are UNK: 0.8500000000000001%\n"
     ]
    }
   ],
   "source": [
    "# Change the text from words to integers\n",
    "# If word is not in vocab, replace it with <UNK> (unknown)\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_headlines = []\n",
    "\n",
    "for date in clean_headlines:\n",
    "    int_daily_headlines = []\n",
    "    for headline in date:\n",
    "        int_headline = []\n",
    "        for word in headline.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                int_headline.append(vocab_to_int[word])\n",
    "            else:\n",
    "                int_headline.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        int_daily_headlines.append(int_headline)\n",
    "    int_headlines.append(int_daily_headlines)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the length of headlines\n",
    "lengths = []\n",
    "for date in int_headlines:\n",
    "    for headline in date:\n",
    "        lengths.append(len(headline))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49693.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.395891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.790246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             counts\n",
       "count  49693.000000\n",
       "mean      12.395891\n",
       "std        6.790246\n",
       "min        1.000000\n",
       "25%        7.000000\n",
       "50%       10.000000\n",
       "75%       16.000000\n",
       "max       41.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the length of a day's news to 200 words, and the length of any headline to 16 words.\n",
    "# These values are chosen to not have an excessively long training time and \n",
    "# balance the number of headlines used and the number of words from each headline.\n",
    "max_headline_length = 16\n",
    "max_daily_length = 200\n",
    "pad_headlines = []\n",
    "\n",
    "for date in int_headlines:\n",
    "    pad_daily_headlines = []\n",
    "    for headline in date:\n",
    "        # Add headline if it is less than max length\n",
    "        if len(headline) <= max_headline_length:\n",
    "            for word in headline:\n",
    "                pad_daily_headlines.append(word)\n",
    "        # Limit headline if it is more than max length  \n",
    "        else:\n",
    "            headline = headline[:max_headline_length]\n",
    "            for word in headline:\n",
    "                pad_daily_headlines.append(word)\n",
    "    \n",
    "    # Pad daily_headlines if they are less than max length\n",
    "    if len(pad_daily_headlines) < max_daily_length:\n",
    "        for i in range(max_daily_length-len(pad_daily_headlines)):\n",
    "            pad = vocab_to_int[\"<PAD>\"]\n",
    "            pad_daily_headlines.append(pad)\n",
    "    # Limit daily_headlines if they are more than max length\n",
    "    else:\n",
    "        pad_daily_headlines = pad_daily_headlines[:max_daily_length]\n",
    "    pad_headlines.append(pad_daily_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize opening prices (target values)\n",
    "max_price = max(price)\n",
    "min_price = min(price)\n",
    "mean_price = np.mean(price)\n",
    "def normalize(price):\n",
    "    return ((price-min_price)/(max_price-min_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_price = []\n",
    "for p in price:\n",
    "    norm_price.append(normalize(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.5448422454901358\n"
     ]
    }
   ],
   "source": [
    "# Check that normalization worked well\n",
    "print(min(norm_price))\n",
    "print(max(norm_price))\n",
    "print(np.mean(norm_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets.\n",
    "# Validating data will be created during training.\n",
    "x_train, x_test, y_train, y_test = train_test_split(pad_headlines, norm_price, test_size = 0.15, random_state = 2)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "# Check the lengths\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.layers import concatenate\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "filter_length1 = 3\n",
    "filter_length2 = 5\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "weights = initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=2)\n",
    "nb_filter = 16\n",
    "rnn_output_size = 128\n",
    "hidden_dims = 128\n",
    "wider = True\n",
    "deeper = True\n",
    "\n",
    "if wider == True:\n",
    "    nb_filter *= 2\n",
    "    rnn_output_size *= 2\n",
    "    hidden_dims *= 2\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    \n",
    "    model1.add(Embedding(nb_words, \n",
    "                         embedding_dim,\n",
    "                         weights=[word_embedding_matrix], \n",
    "                         input_length=max_daily_length))\n",
    "    model1.add(Dropout(dropout))\n",
    "    \n",
    "    model1.add(Convolution1D(filters = nb_filter, \n",
    "                             kernel_size = filter_length1, \n",
    "                             padding = 'same',\n",
    "                            activation = 'relu'))\n",
    "    model1.add(Dropout(dropout))\n",
    "    \n",
    "    if deeper == True:\n",
    "        model1.add(Convolution1D(filters = nb_filter, \n",
    "                                 kernel_size = filter_length1, \n",
    "                                 padding = 'same',\n",
    "                                activation = 'relu'))\n",
    "        model1.add(Dropout(dropout))\n",
    "    \n",
    "    model1.add(LSTM(rnn_output_size, \n",
    "                   activation=None,\n",
    "                   kernel_initializer=weights,\n",
    "                   dropout = dropout))\n",
    "    \n",
    "    ####\n",
    "\n",
    "    model2 = Sequential()\n",
    "    \n",
    "    model2.add(Embedding(nb_words, \n",
    "                         embedding_dim,\n",
    "                         weights=[word_embedding_matrix], \n",
    "                         input_length=max_daily_length))\n",
    "    model2.add(Dropout(dropout))\n",
    "    \n",
    "    \n",
    "    model2.add(Convolution1D(filters = nb_filter, \n",
    "                             kernel_size = filter_length2, \n",
    "                             padding = 'same',\n",
    "                             activation = 'relu'))\n",
    "    model2.add(Dropout(dropout))\n",
    "    \n",
    "    if deeper == True:\n",
    "        model2.add(Convolution1D(filters = nb_filter, \n",
    "                                 kernel_size = filter_length2, \n",
    "                                 padding = 'same',\n",
    "                                 activation = 'relu'))\n",
    "        model2.add(Dropout(dropout))\n",
    "    \n",
    "    model2.add(LSTM(rnn_output_size, \n",
    "                    activation=None,\n",
    "                    kernel_initializer=weights,\n",
    "                    dropout = dropout))\n",
    "    \n",
    "    ####\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #model.add(merge([model1, model2], mode='concat'))\n",
    "    model.add(Concatenate([model1, model2]))\n",
    "    \n",
    "    model.add(Dense(hidden_dims, kernel_initializer=weights))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    if deeper == True:\n",
    "        model.add(Dense(hidden_dims//2, kernel_initializer=weights))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(1, \n",
    "                    kernel_initializer = weights,\n",
    "                    name='output'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(lr=learning_rate,clipvalue=1.0))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current model: Deeper=False, Wider=True, LR=0.001, Dropout=0.3\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-06dba27e15dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                     callbacks = callbacks)\n\u001b[0m",
      "\u001b[0;32m~/src/Predicting-the-Dow-Jones-with-Headlines/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/Predicting-the-Dow-Jones-with-Headlines/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;31m# to match the value shapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/Predicting-the-Dow-Jones-with-Headlines/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;31m# since `Sequential` depends on `Model`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use grid search to help find a better model\n",
    "for deeper in [False]:\n",
    "    for wider in [True,False]:\n",
    "        for learning_rate in [0.001]:\n",
    "            for dropout in [0.3, 0.5]:\n",
    "                model = build_model()\n",
    "                print()\n",
    "                print(\"Current model: Deeper={}, Wider={}, LR={}, Dropout={}\".format(\n",
    "                    deeper,wider,learning_rate,dropout))\n",
    "                print()\n",
    "                save_best_weights = 'question_pairs_weights_deeper={}_wider={}_lr={}_dropout={}.h5'.format(\n",
    "                    deeper,wider,learning_rate,dropout)\n",
    "\n",
    "                callbacks = [ModelCheckpoint(save_best_weights, monitor='val_loss', save_best_only=True),\n",
    "                             EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto'),\n",
    "                             ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=3)]\n",
    "\n",
    "                history = model.fit([x_train,x_train],\n",
    "                                    y_train,\n",
    "                                    batch_size=128,\n",
    "                                    epochs=100,\n",
    "                                    validation_split=0.15,\n",
    "                                    verbose=True,\n",
    "                                    shuffle=True,\n",
    "                                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the best weights\n",
    "deeper=False\n",
    "wider=False\n",
    "dropout=0.3\n",
    "learning_Rate = 0.001\n",
    "# Need to rebuild model in case it is different from the model that was trained most recently.\n",
    "model = build_model()\n",
    "\n",
    "model.load_weights('./question_pairs_weights_deeper={}_wider={}_lr={}_dropout={}.h5'.format(\n",
    "                    deeper,wider,learning_rate,dropout))\n",
    "predictions = model.predict([x_test,x_test], verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare testing loss to training and validating loss\n",
    "mse(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(price):\n",
    "    '''Revert values to their unnormalized amounts'''\n",
    "    price = price*(max_price-min_price)+min_price\n",
    "    return(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnorm_predictions = []\n",
    "for pred in predictions:\n",
    "    unnorm_predictions.append(unnormalize(pred))\n",
    "    \n",
    "unnorm_y_test = []\n",
    "for y in y_test:\n",
    "    unnorm_y_test.append(unnormalize(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median absolute error for the predictions\n",
    "mae(unnorm_y_test, unnorm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary of actual opening price changes\")\n",
    "print(pd.DataFrame(unnorm_y_test, columns=[\"\"]).describe())\n",
    "print()\n",
    "print(\"Summary of predicted opening price changes\")\n",
    "print(pd.DataFrame(unnorm_predictions, columns=[\"\"]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted (blue) and actual (green) values\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(unnorm_predictions)\n",
    "plt.plot(unnorm_y_test)\n",
    "plt.title(\"Predicted (blue) vs Actual (green) Opening Price Changes\")\n",
    "plt.xlabel(\"Testing instances\")\n",
    "plt.ylabel(\"Change in Opening Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to measure if opening price increased or decreased\n",
    "direction_pred = []\n",
    "for pred in unnorm_predictions:\n",
    "    if pred >= 0:\n",
    "        direction_pred.append(1)\n",
    "    else:\n",
    "        direction_pred.append(0)\n",
    "direction_test = []\n",
    "for value in unnorm_y_test:\n",
    "    if value >= 0:\n",
    "        direction_test.append(1)\n",
    "    else:\n",
    "        direction_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate if the predicted direction matched the actual direction\n",
    "direction = acc(direction_test, direction_pred)\n",
    "direction = round(direction,4)*100\n",
    "print(\"Predicted values matched the actual direction {}% of the time.\".format(direction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the data above, this model struggles to accurately predict the change in the opening price of the Dow Jones Instrustial Average. Given that its median average error is 74.15 and the interquartile range of the actual price change is 142.16 (87.47 + 54.69), this model is about as good as someone who knows the average price change of the Dow. \n",
    "\n",
    "I have a few ideas for why this model struggles:\n",
    "- The market is arguably to be a random walk. Although there is some direction to its movements, there is still quite a bit of randomness to its movements.\n",
    "- The news that we used might not be the most relevant. Perhaps it would have been better to use news relating to the 30 companies that make up the Dow.\n",
    "- More information could have been included in the model, such as the previous day(s)'s change, the previous day(s)'s main headline(s). \n",
    "- Many people have worked on this task for years and companies spend millions of dollars to try to predict the movements of the market, so we shouldn't expect anything too great considering the small amount of data that we are working with and the simplicity of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Your Own Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code necessary to make your own predictions. I found that the predictions are most accurate when there is no padding included in the input data. In the create_news variable, I have some default news that you can use, which is from April 30th, 2017. Just change the text to whatever you want, then see the impact your new headline will have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_to_int(news):\n",
    "    '''Convert your created news into integers'''\n",
    "    ints = []\n",
    "    for word in news.split():\n",
    "        if word in vocab_to_int:\n",
    "            ints.append(vocab_to_int[word])\n",
    "        else:\n",
    "            ints.append(vocab_to_int['<UNK>'])\n",
    "    return ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_news(news):\n",
    "    '''Adjusts the length of your created news to fit the model's input values.'''\n",
    "    padded_news = news\n",
    "    if len(padded_news) < max_daily_length:\n",
    "        for i in range(max_daily_length-len(padded_news)):\n",
    "            padded_news.append(vocab_to_int[\"<PAD>\"])\n",
    "    elif len(padded_news) > max_daily_length:\n",
    "        padded_news = padded_news[:max_daily_length]\n",
    "    return padded_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default news that you can use\n",
    "create_news = \"Leaked document reveals Facebook conducted research to target emotionally vulnerable and insecure youth. \\\n",
    "               Woman says note from Chinese 'prisoner' was hidden in new purse. \\\n",
    "               21,000 AT&T workers poised for Monday strike \\\n",
    "               housands march against Trump climate policies in D.C., across USA \\\n",
    "               Kentucky judge won't hear gay adoptions because it's not in the child's \\\"best interest\\\" \\\n",
    "               Multiple victims shot in UTC area apartment complex \\\n",
    "               Drones Lead Police to Illegal Dumping in Riverside County | NBC Southern California \\\n",
    "               An 86-year-old Californian woman has died trying to fight a man who was allegedly sexually assaulting her 61-year-old friend. \\\n",
    "               Fyre Festival Named in $5Million+ Lawsuit after Stranding Festival-Goers on Island with Little Food, No Security. \\\n",
    "               The \\\"Greatest Show on Earth\\\" folds its tent for good \\\n",
    "               U.S.-led fight on ISIS have killed 352 civilians: Pentagon \\\n",
    "               Woman offers undercover officer sex for $25 and some Chicken McNuggets \\\n",
    "               Ohio bridge refuses to fall down after three implosion attempts \\\n",
    "               Jersey Shore MIT grad dies in prank falling from library dome \\\n",
    "               New York graffiti artists claim McDonald's stole work for latest burger campaign \\\n",
    "               SpaceX to launch secretive satellite for U.S. intelligence agency \\\n",
    "               Severe Storms Leave a Trail of Death and Destruction Through the U.S. \\\n",
    "               Hamas thanks N. Korea for its support against â€˜Israeli occupationâ€™ \\\n",
    "               Baker Police officer arrested for allegedly covering up details in shots fired investigation \\\n",
    "               Miami doctorâ€™s call to broker during babyâ€™s delivery leads to $33.8 million judgment \\\n",
    "               Minnesota man gets 15 years for shooting 5 Black Lives Matter protesters \\\n",
    "               South Australian woman facing possible 25 years in Colombian prison for drug trafficking \\\n",
    "               The Latest: Deal reached on funding government through Sept. \\\n",
    "               Russia flaunts Arctic expansion with new military bases\"\n",
    "\n",
    "clean_news = clean_text(create_news)\n",
    "\n",
    "int_news = news_to_int(clean_news)\n",
    "\n",
    "pad_news = padding_news(int_news)\n",
    "\n",
    "pad_news = np.array(pad_news).reshape((1,-1))\n",
    "\n",
    "pred = model.predict([pad_news,pad_news])\n",
    "\n",
    "price_change = unnormalize(pred)\n",
    "\n",
    "print(\"The Dow should open: {} from the previous open.\".format(np.round(price_change[0][0],2)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
